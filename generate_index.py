import os
import shutil
from llama_index.core import (
    SimpleDirectoryReader,
    GPTVectorStoreIndex,
    Settings
)
from llama_index.embeddings.openai import OpenAIEmbedding

# Caminho de saÃ­da
INDEX_DIR = "storage"

# Apaga Ã­ndice antigo (importante!)
if os.path.exists(INDEX_DIR):
    print("ğŸ§¹ Limpando Ã­ndice anterior...")
    shutil.rmtree(INDEX_DIR)

# Carrega a chave da API da OpenAI
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("âŒ OPENAI_API_KEY nÃ£o encontrada nas variÃ¡veis de ambiente.")

# Define o modelo de embedding
Settings.embed_model = OpenAIEmbedding(
    model="text-embedding-3-small",
    api_key=api_key,
)

# LÃª os dados da transcriÃ§Ã£o
print("ğŸ“„ Lendo o arquivo transcricoes.txt...")
documents = SimpleDirectoryReader(input_files=["transcricoes.txt"]).load_data()

# Gera o Ã­ndice
print("âš™ï¸ Gerando o Ã­ndice vetorial...")
index = GPTVectorStoreIndex.from_documents(documents)

# Persiste no diretÃ³rio
print(f"ğŸ’¾ Salvando Ã­ndice em: {INDEX_DIR}")
index.storage_context.persist(persist_dir=INDEX_DIR)

print("âœ… Ãndice criado com sucesso.")
